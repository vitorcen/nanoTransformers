# Mixtral模型训练日志

时间：2025-01-12 01:07:03

## 训练配置

1. 模型配置：
   - 模型类型：Mixtral
   - 隐藏层大小：256
   - 中间层大小：512
   - 注意力头数：8
   - 隐藏层数：4
   - 专家数量：8
   - 每个token使用的专家数：2
   - 最大位置编码：4096
   - 词表大小：32000

2. 训练设置：
   - 训练轮数：3
   - 批次大小：1
   - 学习率：5e-5
   - 梯度累积步骤：8
   - 使用梯度检查点
   - 使用FP16训练

3. 训练数据：
   - 训练集：12个基础文本 * 50 = 600个样本
   - 验证集：4个基础文本 * 25 = 100个样本
   - 最大序列长度：256

4. MoE特定设置：
   - 路由器抖动噪声：0.0
   - 路由器辅助损失系数：0.001
   - 滑动窗口大小：4096

## 训练进度

训练开始...
